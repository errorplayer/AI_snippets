{"cells": [{"metadata": {"id": "yndRSW0ODe9Q", "colab_type": "code", "colab": {}, "trusted": false}, "cell_type": "code", "source": "!dir\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "sweet_preprocessed.pkl\tsweet_pretrained.pth\r\n", "name": "stdout"}]}, {"metadata": {"id": "VmWDE-WHDqYk", "colab_type": "code", "colab": {}, "trusted": false}, "cell_type": "code", "source": "from google.colab import drive\ndrive.mount('/content/drive')", "execution_count": null, "outputs": []}, {"metadata": {"id": "KdwvglZxP5pG", "colab_type": "text"}, "cell_type": "markdown", "source": "#### import required libraries"}, {"metadata": {"id": "aqtco9iKk2Ar", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "outputId": "cee5597d-8035-4f37-e93c-80b47da56718", "trusted": true}, "cell_type": "code", "source": "# !pip install Cython\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport jieba\nimport random\nimport unicodedata\nimport string\nimport re\nfrom sklearn.utils import shuffle\nfrom math import floor, ceil\nimport pickle\nimport os\n\nMAX_LENGTH=10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nBATCH_SIZE=2\n\nENVIR_PATH = ''", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "cuda\n", "name": "stdout"}]}, {"metadata": {"id": "PoKSXWeQP0pR", "colab_type": "text"}, "cell_type": "markdown", "source": "#### chinese text data preprocessing"}, {"metadata": {"id": "ZdMUNyx_7b_6", "colab_type": "code", "colab": {}, "trusted": true}, "cell_type": "code", "source": "class Lang:\n  def __init__(self, name):\n    self.name = name\n    self.word2index = {'S':0, 'P':1}\n    self.word2count = {'S':0, 'P':0}\n    self.index2word = {0:'S', 1:'P'}\n    self.n_words = 2\n\n  def addSentence(self, sentence):\n    for word in sentence.split(' '):\n      self.addWord(word)\n\n  def addWord(self, word):\n    if word not in self.word2index:\n      self.word2index[word] = self.n_words\n      self.word2count[word] = 1\n      self.index2word[self.n_words] = word\n      self.n_words += 1\n    else:\n      self.word2count[word] += 1\n      \n\ndef unicodeToAscii(s):\n  return ''.join(\n      c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn'\n  )\n\ndef sentence_padding(sentence, mode='enc_input', max_length=MAX_LENGTH):\n  ss = sentence.split()\n  sentence_length = len(ss)\n  if mode == 'enc_input' or mode == 1:\n    for i in range(max_length):\n      ss.append('P')\n    return ' '.join(ss[:max_length])\n  if mode == 'dec_input' or mode == 2:\n    for i in range(max_length):\n      ss.append('P')\n    ss.insert(0, 'S')\n    return ' '.join(ss[:max_length])\n  if mode == 'tgt_input' or mode == 3:\n    for i in range(max_length):\n      ss.append('P')\n    return ' '.join(ss[:max_length])\n    \ndef normalizeString_eng(s):\n  s = unicodeToAscii(s.lower().strip())\n  s = re.sub(r'([.!?])', r' \\1', s)\n  s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n  return sentence_padding(s, mode=1)\ndef process_chn_sentence(s, mode):\n  s = ' '.join(jieba.cut(s.split('\\n')[0].replace(' ', '')))\n  return sentence_padding(s, mode)\n\ndef make_batch(language_pairs, batch_size=BATCH_SIZE):\n    tmp_pairs = [random.choice(language_pairs) for n in range(batch_size)]\n    input_batch = [[src_vocab[n] for n in tmp_pair[0].split()] for tmp_pair in tmp_pairs]\n    output_batch = [[tgt_vocab[n] for n in tmp_pair[1].split()] for tmp_pair in tmp_pairs]\n    target_batch = [[tgt_vocab[n] for n in tmp_pair[2].split()] for tmp_pair in tmp_pairs]\n    return torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device), torch.LongTensor(target_batch).to(device)\n\n\n# generate a starting decoder input sequence used for firing the decoder\ndef greedy_decoder(model, enc_input, start_symbol):\n    \"\"\"\n    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n    :param model: Transformer Model\n    :param enc_input: The encoder input\n    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n    :return: The target input\n    \"\"\"\n\n    enc_outputs, enc_self_attns = model.encoder(enc_input)\n    batch_size, seq_len = enc_outputs.size()[0], enc_outputs.size()[1]\n    dec_input = torch.zeros(batch_size, seq_len).type_as(enc_input.data).to(device)\n    next_symbol = start_symbol\n    \n    for i in range(0, seq_len):\n        dec_input[:, i] = next_symbol\n        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n        # dec_outputs [batch_size, seq_len, d_model]\n        # print(dec_outputs.size())\n\n        projected = model.projection(dec_outputs)\n        # projected: [batch_size, seq_len, tgt_vocab_size]\n        # print(projected.size())\n        \n        prob = projected.squeeze(0).max(dim=-1, keepdim=False)\n        # prob:   torch.return_types.max()\n        # prob[0]:  values  [seq_len, ]\n        # prob[1]:  indices  [seq_len, ] every element is the index of the word in tgt_vocab\n        prob = prob[1]\n        next_word = prob.data[i]\n        next_symbol = next_word.data\n    return dec_input\n\ndef PairFilter(pairs):\n  def filterPair(p):\n    return p[0].find('P') > 0 and p[1].find('P') > 0\n  return [pair for pair in pairs if filterPair(pair)]\n\nif not os.path.exists(ENVIR_PATH + 'sweet_preprocessed.pkl'):\n  corpus_path = ENVIR_PATH + 'english-simplified.txt'\n\n  lines = open(corpus_path, encoding='utf-8').read().strip().split('\\n')\n\n  pairs = [[normalizeString_eng(l.split('\\t')[0]), process_chn_sentence(l.split('\\t')[1], 2), process_chn_sentence(l.split('\\t')[1], 3)] for l in lines]\n  filtered_pairs = PairFilter(pairs)\n  print(f'Constraint(s): MAX_LENGTH={MAX_LENGTH}')\n  print('filtered pairs:', len(filtered_pairs))\n  \n  shuffled_pairs = shuffle(filtered_pairs, random_state=88)\n  \n  shuffled_pairs = shuffled_pairs[:50]\n  output_lang = Lang('chinese')\n  input_lang = Lang('english')\n  for pair in shuffled_pairs:\n    input_lang.addSentence(pair[0])\n    output_lang.addSentence(pair[1])\n  print('Counted words:')\n  print(input_lang.name, input_lang.n_words)\n  print(output_lang.name, output_lang.n_words)\n  print(random.choice(filtered_pairs))\n  train_test_ratio = 0.9\n  split_index = floor(len(shuffled_pairs) * train_test_ratio)\n  train_pairs = shuffled_pairs[:split_index]\n  test_pairs = shuffled_pairs[split_index:]\n  print('train_pairs:', split_index)\n  print('test_pairs:', len(shuffled_pairs) - split_index)\n  # filtered_pairs = filtered_pairs[:200]\n\n  big_dict = {'BATCH_SIZE':BATCH_SIZE, 'shuffled_pairs':shuffled_pairs, 'input_lang':input_lang, 'output_lang':output_lang, 'train_pairs':train_pairs, 'test_pairs':test_pairs}\n  f=open(ENVIR_PATH + 'sweet_preprocessed.pkl','wb')\n  pickle.dump(big_dict,f)\n  f.close()\nelse:\n  print('preprocessed data exists!')\n  f=open(ENVIR_PATH + 'sweet_preprocessed.pkl','rb')\n  big_dict=pickle.load(f)\n  input_lang = big_dict['input_lang']\n  output_lang = big_dict['output_lang']\n  train_pairs = big_dict['train_pairs']\n  test_pairs = big_dict['test_pairs']\n  print('load done!')", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "preprocessed data exists!\nload done!\n", "name": "stdout"}]}, {"metadata": {"id": "lsJ2ZyI7kMmv", "colab_type": "text"}, "cell_type": "markdown", "source": "#### model construction"}, {"metadata": {"id": "ABjn_RPKIt05", "colab_type": "code", "scrolled": true, "colab": {}, "trusted": true}, "cell_type": "code", "source": "# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n\n\n\n\nsrc_vocab = input_lang.word2index \nsrc_vocab_size = input_lang.n_words\n\ntgt_vocab = output_lang.word2index \nnumber_dict = output_lang.index2word \ntgt_vocab_size = output_lang.n_words\n\nsrc_len = MAX_LENGTH\ntgt_len = MAX_LENGTH\n\n# Transformer Parameters\nd_model = 128  # Embedding Size\nd_ff = 512 # Position-wise FeedForward dimension\nd_k = d_v = 32  # dimension of K(=Q), V\nn_layers = 4  # number of Encoder of Decoder Layer\nn_heads = 4  # number of heads in Multi-Head Attention\nDROPOUT_PROB = 0.1\n\ndef construc_pos_emb(input_vec):\n  batch_size, seq_len = input_vec.size()\n  a = np.arange(seq_len).reshape(1,seq_len)\n  for row in range(1, batch_size):\n    m = np.arange(seq_len).reshape(1,seq_len)\n    a = np.vstack((a, m))\n  return torch.from_numpy(a).type(torch.LongTensor).to(device)\n\nclass batch_generator(object):\n    def __init__(self, data, batch_size=BATCH_SIZE):\n      self.data = data\n      self.n = len(data)\n      self.current = 0\n      self.batch_size = batch_size\n\n\n    def __iter__(self):\n      return self\n\n    def __next__(self):\n      if self.current < self.n:\n          tmp_pairs = [pair for pair in self.data[self.current : self.current + self.batch_size]]\n          input_batch =  [[src_vocab[n] for n in tmp_pair[0].split()] for tmp_pair in tmp_pairs]\n          output_batch = [[tgt_vocab[n] for n in tmp_pair[1].split()] for tmp_pair in tmp_pairs]\n          target_batch = [[tgt_vocab[n] for n in tmp_pair[2].split()] for tmp_pair in tmp_pairs]  \n          self.current = self.current + self.batch_size\n          res_ = (torch.tensor(input_batch, dtype=torch.long, device=device), torch.tensor(output_batch, dtype=torch.long, device=device), torch.tensor(target_batch, dtype=torch.long, device=device))\n          return res_\n\n      else:\n          raise StopIteration\n\n    def __len__(self):\n      return ceil(self.n/self.batch_size)\n\n\n\ndef get_sinusoid_encoding_table(n_position, d_model):\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n    return torch.FloatTensor(sinusoid_table).to(device)\n\ndef get_attn_pad_mask(seq_q, seq_k):\n    # print(seq_q)\n    # seq_q/seq_k: [batch_size, sentence_length]\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k).to(device)  # batch_size x len_q x len_k\n\ndef get_attn_subsequent_mask(seq):\n    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n    return subsequent_mask.to(device)\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) \n        # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)\n\n        # Fills elements of self tensor with value where mask is one.\n        scores.masked_fill_(attn_mask, -1e9) \n        attn = nn.Softmax(dim=-1)(scores)\n        # attn : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)\n        # V: [batch_size x n_heads x len_q(=len_k) x d_v]\n        context = torch.matmul(attn, V)\n        # context: [batch_size x n_heads x len_q(=len_k) x d_v\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n        self.linear_affin = nn.Linear(n_heads * d_v, d_model)\n        self.layernorm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(DROPOUT_PROB)\n        self.dotproduct  =  ScaledDotProductAttention().to(device)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) \n        # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # q_s: [batch_size x n_heads x len_q x d_k]\n        # k_s: [batch_size x n_heads x len_k x d_k]\n        # v_s: [batch_size x n_heads x len_k x d_v]\n        # scores: (=q_s * k_s^T) [batch_size x n_heads x len_q x len_k]\n        # weighted sum: (=softmax(scores) * v_s) [batch_size x n_heads x len_q x d_v]\n        # weighted sum = context \n        # context: [batch_size x n_heads x len_q x d_v]\n        # attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        # attn = softmax(scores) \n        context, attn = self.dotproduct(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n        \n        # context: [batch_size x len_q x n_heads * d_v]\n        output = self.linear_affin(context)\n        # output: [batch_size x len_q x d_model]\n        output = self.layernorm(output + residual)\n        output = self.dropout(output)\n        return output, attn \n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.layernorm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(DROPOUT_PROB)\n    def forward(self, inputs):\n        # inputs : [batch_size, len_q, d_model]\n        residual = inputs \n        # residual : [batch_size, len_q, d_model]\n        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n        # output: [batch_size, d_ff, len_q]\n        output = self.conv2(output).transpose(1, 2)\n        # output: [batch_size, len_q, d_model]\n        # the two subsequent convolutions is configured with k=1 and padding=zeros,\n        # so, the output sequence won't be changed.\n        output = self.layernorm(output + residual)\n        output = self.dropout(output)\n        return output\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention().to(device)\n        self.pos_ffn = PoswiseFeedForwardNet().to(device)\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n        # enc_inputs: [batch_size x len_q x d_model]\n        # attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        # enc_outputs: [batch_size x len_q x d_model]\n        enc_outputs = self.pos_ffn(enc_outputs) \n        # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass DecoderLayer(nn.Module):\n    def __init__(self):\n        super(DecoderLayer, self).__init__()\n        self.dec_self_attn = MultiHeadAttention().to(device)\n        self.dec_enc_attn = MultiHeadAttention().to(device)\n        self.pos_ffn = PoswiseFeedForwardNet().to(device)\n\n    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n        # the K and V matrix of the encoder-decoder attention layer is generated with the encoder outputs information\n        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n        dec_outputs = self.pos_ffn(dec_outputs)\n        return dec_outputs, dec_self_attn, dec_enc_attn\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([EncoderLayer().to(device) for _ in range(n_layers)])\n\n    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(construc_pos_emb(enc_inputs))\n        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n        enc_self_attns = []\n        for layer in self.layers:\n            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n            enc_self_attns.append(enc_self_attn)\n        return enc_outputs, enc_self_attns\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([DecoderLayer().to(device) for _ in range(n_layers)])\n\n    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(construc_pos_emb(dec_inputs))\n        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n# mask matrix(dec_self_attn_mask) used for decoder-end self attention computation should take the padding information of decoder input \n# and the unsolved mystery of target sentence into account.\n        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n\n        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n\n        dec_self_attns, dec_enc_attns = [], []\n        for layer in self.layers:\n            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n            dec_self_attns.append(dec_self_attn)\n            dec_enc_attns.append(dec_enc_attn)\n        return dec_outputs, dec_self_attns, dec_enc_attns\n\nclass Transformer(nn.Module):\n    def __init__(self):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder().to(device)\n        self.decoder = Decoder().to(device)\n        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n    def forward(self, enc_inputs, dec_inputs):\n        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x seq_len x tgt_vocab_size]\n        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n\n\n\ndef showgraph(attn):\n    attn = attn[-1].squeeze(0)[0]\n    attn = attn.squeeze(0).data.numpy()\n    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n    plt.show()\n\ndef validate(model, valid_generator):\n  model.eval()\n  loss_recorder = 0\n  for data_ele in valid_generator:\n    enc_inputs, dec_inputs, target_batch = data_ele[0], data_ele[1], data_ele[2]\n    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n    target_batch = target_batch.contiguous().view(-1)\n    loss_recorder += float(criterion(outputs, target_batch))\n  return loss_recorder/len(valid_generator)\n\n\nsave_path = ENVIR_PATH + 'sweet_pretrained.pth'\n\n\n", "execution_count": 3, "outputs": []}, {"metadata": {"id": "tDh06gdwemE0", "colab_type": "text"}, "cell_type": "markdown", "source": "#### train"}, {"metadata": {"id": "9f4F3bLQekrT", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "outputId": "91c0056a-8584-4601-d81b-d5b96ae69d91", "trusted": false, "collapsed": true}, "cell_type": "code", "source": "# model = Transformer().to(device)\n# model.load_state_dict(torch.load(save_path))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-09)\n# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,15,50,70], gamma=0.1)\nEPOCH = 100\nBATCH_SIZE=5\n\n\n\nRecorder_loss = {'valid_loss':[], 'train_loss':[]}\n\n# get the current level of training loss\nmodel.eval()\nvalid_gen = batch_generator(test_pairs)\nvalid_loss = validate(model, valid_gen)\nprint('last saved test loss: ', valid_loss)\n# set the starting min loss for model saving\nmin_valid_loss = valid_loss #float('inf')\n\nprint('start training...')\nfor epoch in range(1, EPOCH + 1):\n    torch.cuda.empty_cache()\n    train_gen = batch_generator(train_pairs)\n    valid_gen = batch_generator(test_pairs)\n\n    model.train()\n    optimizer.zero_grad()\n\n    loss_recorder = 0\n    for data_ele in train_gen:\n      enc_inputs, dec_inputs, target_batch = data_ele[0], data_ele[1], data_ele[2] \n      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n      target_batch = target_batch.contiguous().view(-1)\n      # outputs = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"])\n      loss = criterion(outputs, target_batch)\n      loss.backward()\n      optimizer.step()\n      loss_recorder += loss.detach_()\n    train_loss = loss_recorder/len(train_gen)\n    Recorder_loss['train_loss'].append(train_loss)\n\n    # scheduler.step()\n\n    valid_loss = validate(model, valid_gen)\n    Recorder_loss['valid_loss'].append(valid_loss)\n\n    print('Epoch:', '%03d' % (epoch), 'train_loss =', '{:.6f}'.format(train_loss), 'valid_loss =', '{:.6f}'.format(valid_loss))\n    \n    if valid_loss < min_valid_loss:\n      min_valid_loss = valid_loss\n      torch.save(model.state_dict(), save_path)\n \n# torch.save(model.state_dict(), save_path)\n\n", "execution_count": 43, "outputs": [{"output_type": "stream", "text": "last saved test loss:  5.062276522318522\nstart training...\nEpoch: 001 train_loss = 0.676232 valid_loss = 5.334757\nEpoch: 002 train_loss = 3.170341 valid_loss = 5.557751\nEpoch: 003 train_loss = 4.221232 valid_loss = 5.474510\nEpoch: 004 train_loss = 5.379053 valid_loss = 8.375980\nEpoch: 005 train_loss = 6.126348 valid_loss = 10.050239\nEpoch: 006 train_loss = 7.302420 valid_loss = 9.874002\nEpoch: 007 train_loss = 6.219340 valid_loss = 7.726750\nEpoch: 008 train_loss = 4.829407 valid_loss = 7.442229\nEpoch: 009 train_loss = 4.303938 valid_loss = 6.690504\nEpoch: 010 train_loss = 3.927466 valid_loss = 6.473931\nEpoch: 011 train_loss = 3.964869 valid_loss = 6.964736\nEpoch: 012 train_loss = 4.224590 valid_loss = 6.720912\nEpoch: 013 train_loss = 4.416991 valid_loss = 6.842773\nEpoch: 014 train_loss = 4.444931 valid_loss = 7.097198\nEpoch: 015 train_loss = 4.101233 valid_loss = 7.062989\nEpoch: 016 train_loss = 4.028417 valid_loss = 7.045493\nEpoch: 017 train_loss = 4.279836 valid_loss = 6.876467\nEpoch: 018 train_loss = 3.898279 valid_loss = 6.883926\nEpoch: 019 train_loss = 3.787693 valid_loss = 7.057900\nEpoch: 020 train_loss = 3.837988 valid_loss = 7.021052\nEpoch: 021 train_loss = 3.946937 valid_loss = 7.141994\nEpoch: 022 train_loss = 4.031030 valid_loss = 7.197784\nEpoch: 023 train_loss = 3.910286 valid_loss = 7.184686\nEpoch: 024 train_loss = 3.800233 valid_loss = 7.313401\nEpoch: 025 train_loss = 3.775521 valid_loss = 7.427326\nEpoch: 026 train_loss = 3.846194 valid_loss = 7.471352\nEpoch: 027 train_loss = 3.760902 valid_loss = 7.480601\nEpoch: 028 train_loss = 3.785613 valid_loss = 7.698250\nEpoch: 029 train_loss = 3.813463 valid_loss = 7.649148\nEpoch: 030 train_loss = 3.919580 valid_loss = 7.614204\nEpoch: 031 train_loss = 3.847842 valid_loss = 7.655033\nEpoch: 032 train_loss = 3.783375 valid_loss = 7.762936\nEpoch: 033 train_loss = 3.703140 valid_loss = 7.859214\nEpoch: 034 train_loss = 3.712719 valid_loss = 7.885378\nEpoch: 035 train_loss = 3.744433 valid_loss = 7.864624\nEpoch: 036 train_loss = 3.757047 valid_loss = 7.990894\nEpoch: 037 train_loss = 3.719029 valid_loss = 8.110825\nEpoch: 038 train_loss = 3.686813 valid_loss = 8.175374\nEpoch: 039 train_loss = 3.691514 valid_loss = 8.251774\nEpoch: 040 train_loss = 3.684187 valid_loss = 8.236603\nEpoch: 041 train_loss = 3.691776 valid_loss = 8.308473\nEpoch: 042 train_loss = 3.688716 valid_loss = 8.374320\nEpoch: 043 train_loss = 3.727225 valid_loss = 8.469538\nEpoch: 044 train_loss = 3.750116 valid_loss = 8.538207\nEpoch: 045 train_loss = 3.818769 valid_loss = 8.639489\nEpoch: 046 train_loss = 3.718984 valid_loss = 8.599040\nEpoch: 047 train_loss = 3.719437 valid_loss = 8.710186\nEpoch: 048 train_loss = 3.700451 valid_loss = 8.817341\nEpoch: 049 train_loss = 3.666811 valid_loss = 8.794293\nEpoch: 050 train_loss = 3.692500 valid_loss = 8.839745\nEpoch: 051 train_loss = 3.684786 valid_loss = 8.894054\nEpoch: 052 train_loss = 3.734627 valid_loss = 8.992821\nEpoch: 053 train_loss = 3.665886 valid_loss = 9.007061\nEpoch: 054 train_loss = 3.694840 valid_loss = 9.174178\nEpoch: 055 train_loss = 3.753972 valid_loss = 8.691161\nEpoch: 056 train_loss = 3.785766 valid_loss = 9.064102\nEpoch: 057 train_loss = 3.712476 valid_loss = 9.176610\nEpoch: 058 train_loss = 3.689391 valid_loss = 9.020498\nEpoch: 059 train_loss = 3.655501 valid_loss = 9.078835\nEpoch: 060 train_loss = 3.665995 valid_loss = 9.044149\nEpoch: 061 train_loss = 3.642392 valid_loss = 9.155842\nEpoch: 062 train_loss = 3.648340 valid_loss = 9.071253\nEpoch: 063 train_loss = 3.687570 valid_loss = 9.284811\nEpoch: 064 train_loss = 3.734263 valid_loss = 9.100525\nEpoch: 065 train_loss = 3.733014 valid_loss = 9.273513\nEpoch: 066 train_loss = 3.666103 valid_loss = 9.477456\nEpoch: 067 train_loss = 3.654538 valid_loss = 9.339893\nEpoch: 068 train_loss = 3.679087 valid_loss = 9.468879\nEpoch: 069 train_loss = 3.657075 valid_loss = 9.473849\nEpoch: 070 train_loss = 3.651280 valid_loss = 9.567567\nEpoch: 071 train_loss = 3.891771 valid_loss = 9.797501\nEpoch: 072 train_loss = 3.842109 valid_loss = 9.759769\nEpoch: 073 train_loss = 3.788572 valid_loss = 9.968890\nEpoch: 074 train_loss = 3.704986 valid_loss = 9.342308\nEpoch: 075 train_loss = 3.674993 valid_loss = 9.811401\nEpoch: 076 train_loss = 3.729935 valid_loss = 9.554696\nEpoch: 077 train_loss = 3.745627 valid_loss = 9.537177\nEpoch: 078 train_loss = 3.670320 valid_loss = 9.755472\nEpoch: 079 train_loss = 3.647814 valid_loss = 9.561225\nEpoch: 080 train_loss = 3.650471 valid_loss = 9.742249\nEpoch: 081 train_loss = 3.650479 valid_loss = 9.699295\nEpoch: 082 train_loss = 3.642306 valid_loss = 9.749479\nEpoch: 083 train_loss = 3.652988 valid_loss = 9.742359\nEpoch: 084 train_loss = 3.655329 valid_loss = 9.787615\nEpoch: 085 train_loss = 3.675304 valid_loss = 9.448228\nEpoch: 086 train_loss = 3.665457 valid_loss = 9.795672\nEpoch: 087 train_loss = 3.681798 valid_loss = 9.756162\nEpoch: 088 train_loss = 3.634116 valid_loss = 9.686314\nEpoch: 089 train_loss = 3.612451 valid_loss = 9.740316\nEpoch: 090 train_loss = 3.604233 valid_loss = 9.583973\nEpoch: 091 train_loss = 3.560843 valid_loss = 9.532864\nEpoch: 092 train_loss = 3.656773 valid_loss = 10.010553\nEpoch: 093 train_loss = 3.685089 valid_loss = 9.612353\nEpoch: 094 train_loss = 3.700987 valid_loss = 9.679269\nEpoch: 095 train_loss = 3.619129 valid_loss = 9.930519\nEpoch: 096 train_loss = 3.532038 valid_loss = 9.690043\nEpoch: 097 train_loss = 3.593905 valid_loss = 9.681074\nEpoch: 098 train_loss = 3.613485 valid_loss = 9.645903\nEpoch: 099 train_loss = 3.663648 valid_loss = 9.834527\nEpoch: 100 train_loss = 3.654322 valid_loss = 9.850910\n", "name": "stdout"}]}, {"metadata": {"id": "YZZn6lrOeraA", "colab_type": "text"}, "cell_type": "markdown", "source": "#### saving the loss along the training process"}, {"metadata": {"id": "g5m7zvHvpOq1", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 72}, "outputId": "7929165f-da01-43a5-a537-a473458ccf84", "trusted": false}, "cell_type": "code", "source": "import pickle\nf=open(ENVIR_PATH + 'train_loss_log.pkl','wb')\npickle.dump(Recorder_loss,f)\nf.close()", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n", "name": "stderr"}]}, {"metadata": {"id": "7s1NNs6bexjR", "colab_type": "text"}, "cell_type": "markdown", "source": "#### model loading"}, {"metadata": {"id": "tMnxIJAET6KL", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "outputId": "61453627-0148-4c8e-e606-d8b03e01e69a", "trusted": true}, "cell_type": "code", "source": "\n# torch.save(model.state_dict(), save_path)\nmodel = 0\nmodel = Transformer().to(device)\nmodel.load_state_dict(torch.load(save_path))\n", "execution_count": 11, "outputs": []}, {"metadata": {"id": "d23-uBoPPPIW", "colab_type": "text"}, "cell_type": "markdown", "source": "#### scoring the model"}, {"metadata": {"id": "Bw_aURDmPW6W", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "outputId": "85f8218a-cb4b-41a2-c861-9d3998b981b3", "trusted": false}, "cell_type": "code", "source": "# evaluation\nmodel.eval()\nvalid_gen = batch_generator(train_pairs)\nvalid_loss = validate(model, valid_gen)\nvalid_loss", "execution_count": 54, "outputs": [{"output_type": "execute_result", "execution_count": 54, "data": {"text/plain": "0.1985220133488917"}, "metadata": {}}]}, {"metadata": {"id": "GXUbfRvhe0iu", "colab_type": "text"}, "cell_type": "markdown", "source": "#### examinating its performance"}, {"metadata": {"id": "71XeemdyC15e", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 70}, "outputId": "53775f93-738c-432c-c6ca-34feefcb2ea1", "trusted": true}, "cell_type": "code", "source": "\n\n# Test\nenc_inputs, dec_inputs, target_batch = make_batch(train_pairs, batch_size=1)\ngreedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"])\n\n# greedy_dec_input = greedy_dec_input\npredict, _, _, _ = model(enc_inputs, greedy_dec_input)\n# predict [seq_len, tgt_vocab_size]\npredict = predict.data.max(dim=-1, keepdim=True)[1]\n# predict [seq_len, 1]\n\n\nsource_seq = ' '.join([input_lang.index2word[n.item()] for n in enc_inputs[0]]) \noutput_seq1 = ' '.join([number_dict[n.item()] for n in greedy_dec_input.squeeze()])\noutput_seq2 = ' '.join([number_dict[n.item()] for n in predict.squeeze()])\nprint('source:', source_seq)\nprint('result:', output_seq1)\nprint('result:', output_seq2)\n# print([input_lang.index2word[n.item()] for n in enc_inputs[0]], '->', [number_dict[n.item()] for n in predict.squeeze()])\n\n# print('first head of last state enc_self_attns')\n# showgraph(enc_self_attns)\n\n# print('first head of last state dec_self_attns')\n# showgraph(dec_self_attns)\n\n# print('first head of last state dec_enc_attns')\n# showgraph(dec_enc_attns)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "source: i ve always been proud of you . P P\nresult: S \u4f60 \u6700\u597d \u4e0d\u8981 \u5728 \u8fd9\u91cc \u7b49 \u3002 P P\nresult: \u4f60 \u6700\u597d \u4e0d\u8981 \u5728 \u8fd9\u91cc \u7b49 \u3002 P P P\n", "name": "stdout"}]}, {"metadata": {"id": "XX5XTVzroSoT", "colab_type": "text"}, "cell_type": "markdown", "source": "#### have a try on it \n(translate a sentence)"}, {"metadata": {"id": "7mQz9b2yofAQ", "colab_type": "code", "colab": {}, "trusted": true}, "cell_type": "code", "source": "def prepare_single_sentence(sentn):\n  normalized_sentn = normalizeString_eng(sentn)\n  input_batch = [[src_vocab[n] for n in normalized_sentn.split()]]\n  return torch.LongTensor(input_batch).to(device)\n\nsentence_to_be_translated = 'I '\n\n# Test\nenc_inputs = prepare_single_sentence(sentence_to_be_translated)\ngreedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"])\n\n# greedy_dec_input = greedy_dec_input\npredict, _, _, _ = model(enc_inputs, greedy_dec_input)\n# predict [seq_len, tgt_vocab_size]\npredict = predict.data.max(dim=-1, keepdim=True)[1]\n# predict [seq_len, 1]\n\n\nsource_seq = ' '.join([input_lang.index2word[n.item()] for n in enc_inputs[0]]) \noutput_seq1 = ' '.join([number_dict[n.item()] for n in greedy_dec_input.squeeze()])\noutput_seq2 = ' '.join([number_dict[n.item()] for n in predict.squeeze()])\nprint('source:', source_seq)\nprint('result1:', output_seq1)\nprint('result2:', output_seq2)", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "source: i P P P P P P P P P\nresult1: S \u4f60 \u6700\u597d \u4e0d\u8981 \u5728 \u8fd9\u91cc \u7b49 \u3002 P P\nresult2: \u4f60 \u6700\u597d \u4e0d\u8981 \u5728 \u8fd9\u91cc \u7b49 \u3002 P P P\n", "name": "stdout"}]}, {"metadata": {"id": "yRtlif-Ce4ic", "colab_type": "text"}, "cell_type": "markdown", "source": "#### miscellaeous"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "import moxing as mox\n# mox.file.copy('sweet_pretrained.pth', 'obs://class-1275-41781/Lab-2208/mo1/sweet_pretrained.pth')\nmox.file.copy('obs://class-1275-41781/Lab-2208/m91/sweet_pretrained.pth', 'sweet_pretrained.pth')", "execution_count": 52, "outputs": []}, {"metadata": {"id": "_w6_u2XllirC", "colab_type": "code", "colab": {}, "trusted": false}, "cell_type": "code", "source": "import numpy as np\nfrom pprint import pprint\n\nbool_m = np.random.randn(6).reshape(2,3)\npprint(bool_m)\nzero_m = np.zeros((2,3))\nbool_m = bool_m>zero_m\npprint(bool_m)\nvalue_m = np.array([[1,1,0],[0,1,1]])\npprint(value_m)\n\npprint(bool_m+value_m)\n\n\n", "execution_count": null, "outputs": []}], "metadata": {"colab": {"name": "Transformer_implement.ipynb", "provenance": [], "collapsed_sections": [], "toc_visible": true}, "kernelspec": {"name": "pytorch-1.0.0", "display_name": "Pytorch-1.0.0", "language": "python"}, "accelerator": "GPU", "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}